{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5648c19-9f1e-4676-88b1-71a52855f32d",
   "metadata": {},
   "source": [
    "# Loading Data and Setup\n",
    "\n",
    "Here, we load the experiment data and convert the results we have into columns that we can use for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f38c6c1-7324-4fc7-af28-e6c6e6c00fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddbfb917-4a63-459c-a1df-c81cdc4f0082",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/dni138/mozilla_ai/data/flowjudge_run_0_messages_tools_hammerbench_results.json\", \"r\") as f:\n",
    "    first_run = json.load(f)\n",
    "\n",
    "with open(\"/home/dni138/mozilla_ai/data/flowjudge_run_1_messages_tools_hammerbench_results.json\", \"r\") as f:\n",
    "    second_run = json.load(f)\n",
    "\n",
    "with open(\"/home/dni138/mozilla_ai/data/flowjudge_run_2_messages_tools_hammerbench_results.json\", \"r\") as f:\n",
    "    third_run = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a4783a6-ec83-423f-b8c0-4326c3cbe442",
   "metadata": {},
   "outputs": [],
   "source": [
    "hammerbench = pd.read_csv(\"/home/dni138/mozilla_ai/data/hammerbench_singleturn.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "039498bc-9aef-43f3-8c9c-808af459b16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_column = {\n",
    "    \"flowjudge_run_0_messages_tools_hammerbench\": first_run,\n",
    "    \"flowjudge_run_1_messages_tools_hammerbench\": second_run,\n",
    "    \"flowjudge_run_2_messages_tools_hammerbench\": third_run\n",
    "}\n",
    "i = 0\n",
    "\n",
    "for key, data in data_to_column.items():\n",
    "    explanations = []\n",
    "    scores = []\n",
    "    for line in data[key]:\n",
    "        json_line = json.loads(line)\n",
    "        explanations.append(json_line[\"explanation\"])\n",
    "        scores.append(json_line[\"score\"])\n",
    "    hammerbench[\"explanation_run_{}\".format(i)] = explanations\n",
    "    hammerbench[\"score_run_{}\".format(i)] = scores\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "326a7126-ed2c-4334-8d35-db47b0f041a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hammerbench['gt_label'] = [True if label == \"ST-Perfect\" else False for label in hammerbench.label ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d742f993-e753-42fa-b7f4-b2dff798d174",
   "metadata": {},
   "outputs": [],
   "source": [
    "hammerbench.to_csv(\"/home/dni138/mozilla_ai/data/function_call_experiment.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2889fe-543a-4885-86be-80a2106d56cf",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "\n",
    "Here, we look at two different metrics to evaluat how FlowJudge did as a function calling judge. First, we assess how stable FlowJudge during experimentation. We do this by passing the same data points three different times to FlowJudge (i.e. running the experiment three times), and then finding the `cohen_kappa_score` between the three runs pairwise. Second, for each experiment, we check the f1-score, precision, recall, and confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a24a74aa-7032-4b4d-ae31-f0c2f41d20c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix, cohen_kappa_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6393367-1b93-4d7d-b58b-0f50c817749e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hammerbench = pd.read_csv(\"/home/dni138/mozilla_ai/data/function_call_experiment.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "24656593-ef41-424f-8357-3b694e8c9eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "row = hammerbench[hammerbench['label'] == \"ST-Perfect\"].loc[[4418]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a875ade-b139-4e95-8b5f-00b7fe96d386",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>label</th>\n",
       "      <th>messages</th>\n",
       "      <th>tools</th>\n",
       "      <th>explanation_run_0</th>\n",
       "      <th>score_run_0</th>\n",
       "      <th>explanation_run_1</th>\n",
       "      <th>score_run_1</th>\n",
       "      <th>explanation_run_2</th>\n",
       "      <th>score_run_2</th>\n",
       "      <th>gt_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4418</th>\n",
       "      <td>4418</td>\n",
       "      <td>ST-Perfect</td>\n",
       "      <td>[{\"role\": \"user\", \"content\": \"I want to see ho...</td>\n",
       "      <td>[{\"name\": \"VideoPlayback.ShortVideos.playShort...</td>\n",
       "      <td>The function call JSON provided does not meet ...</td>\n",
       "      <td>1</td>\n",
       "      <td>The function call provided in the output does ...</td>\n",
       "      <td>2</td>\n",
       "      <td>The function call JSON provided does not meet ...</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0       label  \\\n",
       "4418        4418  ST-Perfect   \n",
       "\n",
       "                                               messages  \\\n",
       "4418  [{\"role\": \"user\", \"content\": \"I want to see ho...   \n",
       "\n",
       "                                                  tools  \\\n",
       "4418  [{\"name\": \"VideoPlayback.ShortVideos.playShort...   \n",
       "\n",
       "                                      explanation_run_0  score_run_0  \\\n",
       "4418  The function call JSON provided does not meet ...            1   \n",
       "\n",
       "                                      explanation_run_1  score_run_1  \\\n",
       "4418  The function call provided in the output does ...            2   \n",
       "\n",
       "                                      explanation_run_2  score_run_2  gt_label  \n",
       "4418  The function call JSON provided does not meet ...            1      True  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0429649-c68d-4c02-a532-2a474923c905",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The function call JSON provided does not meet the criteria set out in the evaluation rubric. \\n\\n1. **Sufficient Parameters**: The function call JSON for \"Navigation.TrafficViolations.viewViolationDetail\" includes all necessary parameters (plate_number, city, time) to execute the function. This criterion is met.\\n\\n2. **Relevance**: The function call is relevant to the user\\'s query, which is to check a specific traffic violation record. This criterion is met.\\n\\n3. **Existence in Tool List**: The function \"Navigation.TrafficViolations.viewViolationDetail\" does exist in the tool list provided. This criterion is met.\\n\\nHowever, the function call JSON is missing the specific details required for the function to execute properly. The user asked to see how a specific vehicle (plate number \"Su E98765\") handled a speeding record in Nanjing the day before yesterday. While the function call JSON includes the necessary parameters, it does not specify the exact time as \"The day before yesterday,\" which is crucial for the function to retrieve the correct information.\\n\\nTherefore, while the function call meets most of the criteria, it falls short on providing enough specific parameters for execution. This results in a score of 1, as it meets some of the criteria but not all.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(row.explanation_run_0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fab481ea-61e3-4547-92b0-ee25879564b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The function call provided in the output does not meet the criteria specified in the evaluation rubric. \\n\\n1. **Sufficient Parameters**: The function call \"Navigation.TrafficViolations.viewViolationDetail\" has the required parameters (plate_number, city, time) to execute the function.\\n2. **Relevance**: The function call is relevant to the user\\'s query, which is to check the speeding record of a specific vehicle in a specific city on a specific date.\\n3. **Existence in Tool List**: The function \"Navigation.TrafficViolations.viewViolationDetail\" exists in the tool list provided.\\n\\nHowever, the output does not include a tool list. The tool list is provided separately and is not part of the function call. Therefore, the function call meets all the criteria, but the absence of a tool list in the output makes it incomplete.\\n\\nBased on the evaluation criteria and scoring rubric, the function call meets all the necessary conditions, but the lack of a tool list in the output is a significant omission. Therefore, the output should be scored as 2, but with a note regarding the missing tool list.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(row.explanation_run_1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20671888-2e76-4eb5-a11f-dd9efdb29b1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The function call JSON provided does not meet the criteria set out in the evaluation rubric. \\n\\n1. **Sufficient Parameters**: The function call JSON does not provide enough parameters to execute the function. The tool list does not specify any required parameters for the \"Navigation.TrafficViolations.viewViolationDetail\" function.\\n\\n2. **Relevance**: The function call is relevant to the user\\'s ask. The user wants to see the speeding record of a specific vehicle in Nanjing.\\n\\n3. **Existence in Tool List**: The chosen function does exist in the tool list.\\n\\nHowever, since the function call does not provide sufficient parameters, it fails to meet all the criteria. Therefore, it cannot be considered successful based on the rubric.\\n\\nThe correct score, based on the evaluation criteria and scoring rubric, should be 1, as it meets the relevance and existence criteria but not the sufficient parameters criterion.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(row.explanation_run_2)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d879f9d-a090-4038-b681-e7ac229d58a4",
   "metadata": {},
   "source": [
    "## Cohen Kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82ca4f9d-96b5-45ad-a1f9-fa9b9e9dd87e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.26344492678403253"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cohen_kappa_score(hammerbench.score_run_0, hammerbench.score_run_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9ffa005-28c1-404c-bf6d-5daba5e7a838",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.27004185295065264"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cohen_kappa_score(hammerbench.score_run_1, hammerbench.score_run_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c34e5abc-c977-4e6d-a96e-ec25679e69e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.27486275759596934"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cohen_kappa_score(hammerbench.score_run_0, hammerbench.score_run_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd058e3e-acb1-4eb9-80a6-88e73f4c3a3c",
   "metadata": {},
   "source": [
    "## Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7437008a-2a47-4df0-9f7b-417b66c46325",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['label', 'messages', 'tools', 'explanation_run_0', 'score_run_0',\n",
       "       'explanation_run_1', 'score_run_1', 'explanation_run_2', 'score_run_2',\n",
       "       'gt_label'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hammerbench.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "70fcc7fe-8738-4089-a3d4-621a9abbc145",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_to_bool = {\n",
    "    0: False,\n",
    "    1: False,\n",
    "    2: True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2771f038-429e-438a-900e-70982f3521e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25142/2427440671.py:1: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  hammerbench[\"score_run_0\"].map(score_to_bool).fillna(False)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0        False\n",
       "1        False\n",
       "2        False\n",
       "3        False\n",
       "4        False\n",
       "         ...  \n",
       "13049    False\n",
       "13050    False\n",
       "13051    False\n",
       "13052    False\n",
       "13053    False\n",
       "Name: score_run_0, Length: 13054, dtype: bool"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hammerbench[\"score_run_0\"].map(score_to_bool).fillna(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8730b022-2026-4d9f-a3b1-d77d1439d875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------RUN_0-----------\n",
      "F1 Score: 0.5035039230266493\n",
      "Precision: 0.6131466994997525\n",
      "Recall: 0.5196778041969499\n",
      "Confusion Matrix: \n",
      "\n",
      " [[10743   195]\n",
      " [ 1995   121]] \n",
      "\n",
      "----------RUN_1-----------\n",
      "F1 Score: 0.5046584311020123\n",
      "Precision: 0.6251617793110659\n",
      "Recall: 0.5206912064252164\n",
      "Confusion Matrix: \n",
      "\n",
      " [[10760   178]\n",
      " [ 1994   122]] \n",
      "\n",
      "----------RUN_2-----------\n",
      "F1 Score: 0.49889597981284217\n",
      "Precision: 0.6054650923850642\n",
      "Recall: 0.5172079630126981\n",
      "Confusion Matrix: \n",
      "\n",
      " [[10751   187]\n",
      " [ 2007   109]] \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25142/741388246.py:3: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  print(\"F1 Score: {}\".format(f1_score(hammerbench.gt_label, hammerbench[\"score_run_{}\".format(i)].map(score_to_bool).fillna(False), average=\"macro\", labels=[False, True])))\n",
      "/tmp/ipykernel_25142/741388246.py:4: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  print(\"Precision: {}\".format(precision_score(hammerbench.gt_label, hammerbench[\"score_run_{}\".format(i)].map(score_to_bool).fillna(False), average=\"macro\", labels=[False, True])))\n",
      "/tmp/ipykernel_25142/741388246.py:5: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  print(\"Recall: {}\".format(recall_score(hammerbench.gt_label, hammerbench[\"score_run_{}\".format(i)].map(score_to_bool).fillna(False), average=\"macro\", labels=[False, True])))\n",
      "/tmp/ipykernel_25142/741388246.py:6: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  print(\"Confusion Matrix: \\n\\n {} \\n\".format(confusion_matrix(hammerbench.gt_label, hammerbench[\"score_run_{}\".format(i)].map(score_to_bool).fillna(False), labels=[False, True])))\n",
      "/tmp/ipykernel_25142/741388246.py:3: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  print(\"F1 Score: {}\".format(f1_score(hammerbench.gt_label, hammerbench[\"score_run_{}\".format(i)].map(score_to_bool).fillna(False), average=\"macro\", labels=[False, True])))\n",
      "/tmp/ipykernel_25142/741388246.py:4: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  print(\"Precision: {}\".format(precision_score(hammerbench.gt_label, hammerbench[\"score_run_{}\".format(i)].map(score_to_bool).fillna(False), average=\"macro\", labels=[False, True])))\n",
      "/tmp/ipykernel_25142/741388246.py:5: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  print(\"Recall: {}\".format(recall_score(hammerbench.gt_label, hammerbench[\"score_run_{}\".format(i)].map(score_to_bool).fillna(False), average=\"macro\", labels=[False, True])))\n",
      "/tmp/ipykernel_25142/741388246.py:6: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  print(\"Confusion Matrix: \\n\\n {} \\n\".format(confusion_matrix(hammerbench.gt_label, hammerbench[\"score_run_{}\".format(i)].map(score_to_bool).fillna(False), labels=[False, True])))\n",
      "/tmp/ipykernel_25142/741388246.py:3: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  print(\"F1 Score: {}\".format(f1_score(hammerbench.gt_label, hammerbench[\"score_run_{}\".format(i)].map(score_to_bool).fillna(False), average=\"macro\", labels=[False, True])))\n",
      "/tmp/ipykernel_25142/741388246.py:4: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  print(\"Precision: {}\".format(precision_score(hammerbench.gt_label, hammerbench[\"score_run_{}\".format(i)].map(score_to_bool).fillna(False), average=\"macro\", labels=[False, True])))\n",
      "/tmp/ipykernel_25142/741388246.py:5: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  print(\"Recall: {}\".format(recall_score(hammerbench.gt_label, hammerbench[\"score_run_{}\".format(i)].map(score_to_bool).fillna(False), average=\"macro\", labels=[False, True])))\n",
      "/tmp/ipykernel_25142/741388246.py:6: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  print(\"Confusion Matrix: \\n\\n {} \\n\".format(confusion_matrix(hammerbench.gt_label, hammerbench[\"score_run_{}\".format(i)].map(score_to_bool).fillna(False), labels=[False, True])))\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(\"----------RUN_{}-----------\".format(i))\n",
    "    print(\"F1 Score: {}\".format(f1_score(hammerbench.gt_label, hammerbench[\"score_run_{}\".format(i)].map(score_to_bool).fillna(False), average=\"macro\", labels=[False, True])))\n",
    "    print(\"Precision: {}\".format(precision_score(hammerbench.gt_label, hammerbench[\"score_run_{}\".format(i)].map(score_to_bool).fillna(False), average=\"macro\", labels=[False, True])))\n",
    "    print(\"Recall: {}\".format(recall_score(hammerbench.gt_label, hammerbench[\"score_run_{}\".format(i)].map(score_to_bool).fillna(False), average=\"macro\", labels=[False, True])))\n",
    "    print(\"Confusion Matrix: \\n\\n {} \\n\".format(confusion_matrix(hammerbench.gt_label, hammerbench[\"score_run_{}\".format(i)].map(score_to_bool).fillna(False), labels=[False, True])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28415449-9b21-4c7d-a5fc-39296b14be31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
